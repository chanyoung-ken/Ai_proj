{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# 랜덤 시드 설정\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCDropout3DCNN(tf.keras.Model):\n",
    "    def __init__(self, input_shape, num_classes=5, dropout_rate=0.3):\n",
    "        super(MCDropout3DCNN, self).__init__()\n",
    "        \n",
    "        # 첫 번째 3D 합성곱 블록\n",
    "        self.conv1_1 = layers.Conv3D(32, kernel_size=3, activation='relu', padding='same')\n",
    "        self.bn1_1 = layers.BatchNormalization()\n",
    "        self.conv1_2 = layers.Conv3D(32, kernel_size=3, activation='relu', padding='same')\n",
    "        self.bn1_2 = layers.BatchNormalization()\n",
    "        self.pool1 = layers.MaxPool3D(pool_size=2)\n",
    "        self.drop1 = layers.Dropout(dropout_rate)\n",
    "        \n",
    "        # 두 번째 3D 합성곱 블록\n",
    "        self.conv2_1 = layers.Conv3D(64, kernel_size=3, activation='relu', padding='same')\n",
    "        self.bn2_1 = layers.BatchNormalization()\n",
    "        self.conv2_2 = layers.Conv3D(64, kernel_size=3, activation='relu', padding='same')\n",
    "        self.bn2_2 = layers.BatchNormalization()\n",
    "        self.pool2 = layers.MaxPool3D(pool_size=2)\n",
    "        self.drop2 = layers.Dropout(dropout_rate)\n",
    "        \n",
    "        # 세 번째 3D 합성곱 블록\n",
    "        self.conv3_1 = layers.Conv3D(128, kernel_size=3, activation='relu', padding='same',\n",
    "                           kernel_regularizer=regularizers.l2(1e-4))\n",
    "        self.bn3_1 = layers.BatchNormalization()\n",
    "        self.conv3_2 = layers.Conv3D(128, kernel_size=3, activation='relu', padding='same',\n",
    "                           kernel_regularizer=regularizers.l2(1e-4))\n",
    "        self.bn3_2 = layers.BatchNormalization()\n",
    "        self.pool3 = layers.MaxPool3D(pool_size=2)\n",
    "        self.drop3 = layers.Dropout(dropout_rate)\n",
    "        \n",
    "        # 완전 연결 계층\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.dense1 = layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(1e-4))\n",
    "        self.bn4 = layers.BatchNormalization()\n",
    "        self.drop4 = layers.Dropout(dropout_rate)\n",
    "        self.dense2 = layers.Dense(num_classes, activation='softmax')\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        # 첫 번째 블록\n",
    "        x = self.conv1_1(inputs)\n",
    "        x = self.bn1_1(x, training=training)\n",
    "        x = self.conv1_2(x)\n",
    "        x = self.bn1_2(x, training=training)\n",
    "        x = self.pool1(x)\n",
    "        x = self.drop1(x, training=training)\n",
    "        \n",
    "        # 두 번째 블록\n",
    "        x = self.conv2_1(x)\n",
    "        x = self.bn2_1(x, training=training)\n",
    "        x = self.conv2_2(x)\n",
    "        x = self.bn2_2(x, training=training)\n",
    "        x = self.pool2(x)\n",
    "        x = self.drop2(x, training=training)\n",
    "        \n",
    "        # 세 번째 블록\n",
    "        x = self.conv3_1(x)\n",
    "        x = self.bn3_1(x, training=training)\n",
    "        x = self.conv3_2(x)\n",
    "        x = self.bn3_2(x, training=training)\n",
    "        x = self.pool3(x)\n",
    "        x = self.drop3(x, training=training)\n",
    "        \n",
    "        # 완전 연결 계층\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.bn4(x, training=training)\n",
    "        x = self.drop4(x, training=training)\n",
    "        x = self.dense2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def build_graph(self):\n",
    "        x = tf.keras.Input(shape=(16, 32, 32, 1))\n",
    "        return tf.keras.Model(inputs=[x], outputs=self.call(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (16, 32, 32, 1)  # (depth, height, width, channels)\n",
    "mc_dropout_model = MCDropout3DCNN(input_shape)\n",
    "mc_dropout_graph = mc_dropout_model.build_graph()\n",
    "print(\"MC Dropout 모델 요약:\")\n",
    "mc_dropout_graph.summary()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_dropout_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_dropout_predict(model, X, num_samples=30):\n",
    "    predictions = []\n",
    "    \n",
    "    # 학습 모드로 설정하여 드롭아웃 활성화\n",
    "    for _ in range(num_samples):\n",
    "        y_pred = model(X, training=True)\n",
    "        predictions.append(y_pred.numpy())\n",
    "    \n",
    "    # 예측값의 평균과 표준편차\n",
    "    mean_prediction = np.mean(predictions, axis=0)\n",
    "    std_prediction = np.std(predictions, axis=0)\n",
    "    \n",
    "    return mean_prediction, std_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_uncertainty(mean_pred, std_pred, sample_idx=0):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # 각 클래스별 예측 확률과 불확실성\n",
    "    plt.subplot(1, 2, 1)\n",
    "    num_classes = mean_pred.shape[1]\n",
    "    classes = range(num_classes)\n",
    "    \n",
    "    plt.bar(classes, mean_pred[sample_idx], yerr=std_pred[sample_idx], capsize=10)\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.title('Prediction Probability with Uncertainty')\n",
    "    plt.xticks(classes)\n",
    "    \n",
    "    # 불확실성 분포\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(np.mean(std_pred, axis=1), bins=30)\n",
    "    plt.axvline(std_pred[sample_idx].mean(), color='r', linestyle='--', \n",
    "                label=f'Sample #{sample_idx} uncertainty')\n",
    "    plt.xlabel('Mean Uncertainty (Std)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Uncertainty Distribution')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_uncertainty_vs_accuracy(mean_pred, std_pred, y_true):\n",
    "    # 예측 클래스와 실제 클래스\n",
    "    pred_classes = np.argmax(mean_pred, axis=1)\n",
    "    true_classes = np.argmax(y_true, axis=1)\n",
    "    \n",
    "    # 정확한 예측과 잘못된 예측 구분\n",
    "    correct = pred_classes == true_classes\n",
    "    \n",
    "    # 각 샘플의 평균 불확실성\n",
    "    mean_uncertainty = np.mean(std_pred, axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(mean_uncertainty[correct], np.ones(np.sum(correct)), \n",
    "                label='Correct Predictions', alpha=0.5, color='blue')\n",
    "    plt.scatter(mean_uncertainty[~correct], np.zeros(np.sum(~correct)), \n",
    "                label='Wrong Predictions', alpha=0.5, color='red')\n",
    "    \n",
    "    plt.xlabel('Mean Uncertainty (Std)')\n",
    "    plt.ylabel('Prediction Correctness')\n",
    "    plt.yticks([0, 1], ['Wrong', 'Correct'])\n",
    "    plt.title('Relationship Between Uncertainty and Prediction Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, X_val, y_val, epochs=50, batch_size=16):\n",
    "    # 콜백 정의\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # 모델 학습\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_dropout_model.save('mc_dropout_3dcnn_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # 정확도 시각화\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # 손실 시각화\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_with_uncertainty(model, X_test, y_test):\n",
    "    # MC Dropout을 사용한 베이지안 추론\n",
    "    mean_pred, std_pred = mc_dropout_predict(model, X_test)\n",
    "    pred_classes = np.argmax(mean_pred, axis=1)\n",
    "    true_classes = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    # 정확도 계산\n",
    "    accuracy = accuracy_score(true_classes, pred_classes)\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # 혼동 행렬\n",
    "    conf_matrix = confusion_matrix(true_classes, pred_classes)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 분류 보고서\n",
    "    report = classification_report(true_classes, pred_classes)\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "    \n",
    "    # 불확실성 시각화\n",
    "    plot_uncertainty(mean_pred, std_pred)\n",
    "    \n",
    "    # 불확실성과 정확도 관계 분석\n",
    "    plot_uncertainty_vs_accuracy(mean_pred, std_pred, y_test)\n",
    "    \n",
    "    return mean_pred, std_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model('mc_dropout_3dcnn_model')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
